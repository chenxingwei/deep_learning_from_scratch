# Neural networks with single neuron

Neural network with single neuron is the simplest neural networks, and to some extent, all neural network models can be consist of multiple single neurons.

## Introduction

We have previously introduce the basic concepts of neuron in [Neuron](https://github.com/chenxingwei/deep_learning_from_scratch/blob/master/algorithm/1.neurons.md) section. And it is quite simple to model the neural into a mathematical model:

<img src="https://github.com/chenxingwei/deep_learning_from_scratch/blob/master/images/neuron003.png" width="80%" height="80%">

The model above is the single neuron neural network. Based on the type of activation functions, we can have different models, such as sigmoid activation function, ReLU activation function, and so on. Here I will only introduce models with sigmoid and ReLU activation function.

If the activation function is ![equation](http://latex.codecogs.com/gif.latex?\sigma(x)=x), then the single neuron problem is the [Linear Regression](https://github.com/chenxingwei/machine_learning_from_scratch/blob/master/algorithm/1.linearRegression.md) problem.

### Neural network with single neuron (Sigmoid activation function)

We first introduce the sigmoid function, which is expressed as:

![equation](http://latex.codecogs.com/gif.latex?\sigma(x)=\frac{1}{1+e^{-x}})

The curve of above function can be visualized below:

<img src="https://github.com/chenxingwei/deep_learning_from_scratch/blob/master/images/004.png" width="40%" height="40%">

It is quite simple because it is actually the [Logistic Regression](https://github.com/chenxingwei/machine_learning_from_scratch/blob/master/algorithm/3.logisticRegression.md) which we have already introduced in the machine learning part. 

### Neural network with single neuron (ReLU activation function)

Rectified Linear Unit (ReLU) activation function is very popular in recent convolutional neural networks, which is expressed as:

![equation](http://latex.codecogs.com/gif.latex?\sigma(x)=\left\\{\begin{matrix}x,x\ge{0}\\\0,x<{0}\end{matrix}\right.)

The function curve of ReLU activation function can be visualized below:

<img src="https://github.com/chenxingwei/deep_learning_from_scratch/blob/master/images/005.png" width="40%" height="40%">


## Summary

